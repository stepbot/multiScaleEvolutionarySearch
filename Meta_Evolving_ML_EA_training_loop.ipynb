{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepbot/multiScaleEvolutionarySearch/blob/master/Meta_Evolving_ML_EA_training_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Meta-Evolving Genetic Algorithm Operators with LLMs (V2)\n",
        "\n",
        "This script implements a meta-evolutionary algorithm where the core evolutionary\n",
        "operator itself is evolved by an LLM.\n",
        "\n",
        "Instead of evolving separate 'crossover' and 'mutate' functions, we evolve a\n",
        "single, holistic function: `generate_next_population`. This function takes the\n",
        "current population and their fitness scores and is responsible for producing\n",
        "the entire next generation.\n",
        "\n",
        "This approach gives the LLM maximum creative freedom to discover novel\n",
        "evolutionary strategies, potentially departing from traditional GA structures.\n",
        "\"\"\"\n",
        "\n",
        "# @title 1. Install Dependencies\n",
        "!pip install -q -U google-generativeai openai tqdm matplotlib\n",
        "\n",
        "# @title 2. Setup and Imports\n",
        "import google.generativeai as genai\n",
        "import openai\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import textwrap\n",
        "from IPython.display import Markdown, display\n",
        "import itertools\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "# @title 3. Configure LLM APIs\n",
        "# @markdown Add your API keys to the Colab secrets manager (key icon on the left).\n",
        "# @markdown - `GOOGLE_API_KEY` for Gemini\n",
        "# @markdown - `OPENAI_API_KEY` for OpenAI\n",
        "# @markdown The script will use any and all configured clients.\n",
        "\n",
        "llm_clients = []\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # Configure Gemini\n",
        "    GOOGLE_API_KEY = userdata.get('gemini_key')\n",
        "    if GOOGLE_API_KEY:\n",
        "        genai.configure(api_key=GOOGLE_API_KEY)\n",
        "        # Use the updated model name here\n",
        "        llm_clients.append(genai.GenerativeModel('gemini-2.5-pro'))\n",
        "        print(\"✅ Successfully configured and added Gemini client.\")\n",
        "\n",
        "        # --- Debug: List available Gemini models ---\n",
        "        print(\"\\n--- Available Gemini Models (for 'generateContent') ---\")\n",
        "        # The genai.list_models() function gets all models\n",
        "        for m in genai.list_models():\n",
        "            if 'generateContent' in m.supported_generation_methods:\n",
        "                print(m.name)\n",
        "        print(\"-----------------------------------------------------\\n\")\n",
        "        # --- End Debug ---\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Warning: GOOGLE_API_KEY not found in Colab secrets. Gemini client will not be used.\")\n",
        "\n",
        "    # Configure OpenAI\n",
        "    OPENAI_API_KEY = userdata.get('openai_key')\n",
        "    if OPENAI_API_KEY:\n",
        "        # It's good practice to set the env variable for some libraries\n",
        "        os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "        openai_client = openai.OpenAI() # Create a client instance first\n",
        "        llm_clients.append(openai_client)\n",
        "        print(\"✅ Successfully configured and added OpenAI client.\")\n",
        "\n",
        "        # --- Debug: List available OpenAI models ---\n",
        "        print(\"\\n--- Available OpenAI Models (GPT models) ---\")\n",
        "        try:\n",
        "            # The client.models.list() function gets all models\n",
        "            model_list = [m.id for m in openai_client.models.list() if 'gpt' in m.id.lower()]\n",
        "            for model_name in sorted(model_list):\n",
        "                print(model_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not retrieve OpenAI models: {e}\")\n",
        "        print(\"-------------------------------------------\\n\")\n",
        "        # --- End Debug ---\n",
        "\n",
        "    else:\n",
        "        print(\"⚠️ Warning: OPENAI_API_KEY not found in Colab secrets. OpenAI client will not be used.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"❌ Could not import userdata. Please run this in a Google Colab environment and add API keys to secrets.\")\n",
        "\n",
        "def call_llm_api(client, prompt, model_name_gemini='gemini-1.5-pro-latest', model_name_openai='gpt-4o'):\n",
        "    \"\"\"A unified function to call either Gemini or OpenAI API.\"\"\"\n",
        "    client_name = client.__class__.__module__.split('.')[0]\n",
        "    if client_name == 'google':\n",
        "        model = genai.GenerativeModel(model_name_gemini)\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    elif client_name == 'openai':\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name_openai,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that only returns Python code.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown client type: {client_name}\")\n",
        "\n",
        "\n",
        "# @title 4. Define the Machine Learning Problem (Inner Loop)\n",
        "\n",
        "# --- Configuration ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_SIZE = 28 * 28\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# --- Model Architecture ---\n",
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"A simple MLP suitable for MNIST, used in the inner GA.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(INPUT_SIZE, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, INPUT_SIZE)\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# --- Data Loading & Evaluation ---\n",
        "def get_data_loaders():\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate_nn_model(model, loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# @title 5. Define the Meta-Evolutionary Algorithm (Outer Loop)\n",
        "\n",
        "class OperatorIndividual:\n",
        "    \"\"\"\n",
        "    Represents a single evolutionary operator, now with detailed,\n",
        "    multi-run, per-generation performance history.\n",
        "    \"\"\"\n",
        "    def __init__(self, evolution_operator_code):\n",
        "        self.evolution_operator_code = evolution_operator_code\n",
        "        self.run_histories = [] # Stores detailed history from multiple runs\n",
        "        self.fitness = 0.0      # A single representative score for sorting/selection\n",
        "\n",
        "    def update_history(self, new_run_histories):\n",
        "        \"\"\"Adds new run histories and recalculates the primary fitness score.\"\"\"\n",
        "        if not new_run_histories:\n",
        "            return\n",
        "        self.run_histories.extend(new_run_histories)\n",
        "        self._calculate_primary_fitness()\n",
        "\n",
        "    def _calculate_primary_fitness(self):\n",
        "        \"\"\"\n",
        "        Calculates a single fitness score for elitism and selection.\n",
        "        We define this as the average of the 'best' fitness from the\n",
        "        final generation of each run. This rewards operators that\n",
        "        consistently produce high-performing individuals.\n",
        "        \"\"\"\n",
        "        if not self.run_histories:\n",
        "            self.fitness = 0.0\n",
        "            return\n",
        "\n",
        "        final_bests = [run[-1]['best'] for run in self.run_histories if run]\n",
        "        if final_bests:\n",
        "            self.fitness = sum(final_bests) / len(final_bests)\n",
        "        else:\n",
        "            self.fitness = 0.0\n",
        "\n",
        "    def get_performance_summary_text(self):\n",
        "        \"\"\"\n",
        "        Processes the detailed run histories into a clean, averaged,\n",
        "        human-readable summary table for the LLM prompt.\n",
        "        \"\"\"\n",
        "        if not self.run_histories:\n",
        "            return \"This is a new operator that has not been evaluated yet. Your goal is to create a strong initial version.\"\n",
        "\n",
        "        # Average the stats across all runs to create a single, clear time-series\n",
        "        num_runs = len(self.run_histories)\n",
        "        # Ensure all runs have the same number of generations\n",
        "        if not all(len(run) == len(self.run_histories[0]) for run in self.run_histories):\n",
        "             return \"Error: Inconsistent history data.\"\n",
        "        num_gens = len(self.run_histories[0])\n",
        "\n",
        "        avg_history = [{\"best\": 0.0, \"avg\": 0.0, \"worst\": 0.0} for _ in range(num_gens)]\n",
        "\n",
        "        for run in self.run_histories:\n",
        "            for i in range(num_gens):\n",
        "                avg_history[i][\"best\"] += run[i][\"best\"]\n",
        "                avg_history[i][\"avg\"] += run[i][\"avg\"]\n",
        "                avg_history[i][\"worst\"] += run[i][\"worst\"]\n",
        "\n",
        "        summary_lines = [\n",
        "            f\"This operator's performance has been recorded over **{num_runs}** separate runs.\",\n",
        "            \"The table below shows the fitness dynamics, **averaged across all runs**.\",\n",
        "            \"Fitness is based on the negative loss on training batches (higher is better).\\n\",\n",
        "            \"| Gen | Best Fitness | Avg Fitness  | Worst Fitness | Spread (Diversity) |\",\n",
        "            \"|:---:|:------------:|:------------:|:-------------:|:------------------:|\",\n",
        "        ]\n",
        "\n",
        "        for i in range(num_gens):\n",
        "            gen_stats = avg_history[i]\n",
        "            avg_best = gen_stats['best'] / num_runs\n",
        "            avg_avg = gen_stats['avg'] / num_runs\n",
        "            avg_worst = gen_stats['worst'] / num_runs\n",
        "            spread = avg_best - avg_worst\n",
        "            summary_lines.append(f\"| {i:^3} | {avg_best:12.4f} | {avg_avg:12.4f} | {avg_worst:13.4f} | {spread:18.4f} |\")\n",
        "\n",
        "        summary_lines.extend([\n",
        "            \"\\n**Analysis Hints for Your Evolution:**\",\n",
        "            \"- **Rate of Improvement:** Analyze the slope of the `Best Fitness` column. A steep, consistent increase is ideal.\",\n",
        "            \"- **Population Diversity:** The `Spread (Best-Worst)` column is a proxy for diversity. If it collapses to near-zero too quickly, the population has prematurely converged, and you should consider changes that increase exploration (e.g., higher mutation, different selection).\",\n",
        "            \"- **Stability:** Smooth, predictable improvements indicate a stable operator. Jagged or erratic values might suggest the operator is too chaotic.\"\n",
        "        ])\n",
        "\n",
        "        return \"\\n\".join(summary_lines)\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Provides a compact summary for console logging.\"\"\"\n",
        "        if not self.run_histories:\n",
        "            return f\"Fitness: Not yet evaluated\\n--- Operator Code ---\\n{self.evolution_operator_code}\"\n",
        "        return (f\"Overall Fitness (Avg Final Best): {self.fitness:.4f}\\n\"\n",
        "                f\"Evaluated over {len(self.run_histories)} runs.\\n\"\n",
        "                f\"--- Operator Code ---\\n{self.evolution_operator_code}\")\n",
        "\n",
        "\n",
        "def selection(population, tournament_size=3):\n",
        "    \"\"\"Selects a parent from the population using tournament selection based on its primary fitness score.\"\"\"\n",
        "    if len(population) < tournament_size:\n",
        "        return random.choice(population)\n",
        "    tournament = random.sample(population, tournament_size)\n",
        "    # The 'fitness' attribute now represents consistent high performance\n",
        "    tournament.sort(key=lambda x: x.fitness, reverse=True)\n",
        "    return tournament[0]\n",
        "\n",
        "def llm_evolve_operator(operator_individual, clients):\n",
        "    \"\"\"Uses a randomly selected LLM client to evolve the evolutionary operator, providing detailed generational feedback.\"\"\"\n",
        "    if not clients:\n",
        "        print(\"Warning: No LLM clients configured. Returning original operator.\")\n",
        "        return OperatorIndividual(operator_individual.evolution_operator_code)\n",
        "\n",
        "    client = random.choice(clients)\n",
        "    client_name = client.__class__.__module__.split('.')[0]\n",
        "\n",
        "    # ✨ NEW: Generate the detailed performance summary ✨\n",
        "    performance_feedback = operator_individual.get_performance_summary_text()\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert in genetic algorithms. Your task is to evolve a Python function that acts as a holistic evolutionary operator.\n",
        "This function, `generate_next_population`, is the complete engine of a genetic algorithm, responsible for selection, crossover, and mutation to create the next generation of neural networks.\n",
        "\n",
        "**Performance Feedback:**\n",
        "{performance_feedback}\n",
        "\n",
        "Based on these detailed generational dynamics, your goal is to generate a new, improved version of the operator code.\n",
        "- If the operator shows **good, stable improvement**, consider a subtle refinement.\n",
        "- If the operator **stagnates or converges too fast** (low spread), consider changes that increase exploration or diversity.\n",
        "- If the operator is **unstable or performs poorly**, a more radical change to the evolutionary strategy might be needed.\n",
        "\n",
        "The function signature MUST BE `def generate_next_population(current_population, fitness_scores, device, torch, SimpleNet):`.\n",
        "It must return a new list of models of the same size as the input population.\n",
        "\n",
        "**Current Operator Code:**\n",
        "```python\n",
        "{operator_individual.evolution_operator_code}\n",
        "```\n",
        "Return only the Python code block for the new, evolved function. Do not include explanations or markdown formatting.\n",
        "\"\"\"\n",
        "    try:\n",
        "        print(f\"--- Calling {client_name.capitalize()} API to evolve operator (with generational feedback) ---\")\n",
        "        if client_name == 'google':\n",
        "            # Updated model name for best performance\n",
        "            model = genai.GenerativeModel('gemini-2.5-pro')\n",
        "            response = model.generate_content(prompt)\n",
        "            evolved_code = response.text\n",
        "        elif client_name == 'openai':\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-5\", # Use a strong model for this complex task\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that only returns Python code.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            evolved_code = response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: API call to {client_name.capitalize()} failed: {e}. Returning original operator.\")\n",
        "\n",
        "    return OperatorIndividual(evolved_code)\n",
        "\n",
        "def repair_operator_with_llm(faulty_code, error_trace, clients):\n",
        "    \"\"\"\n",
        "    Attempts to repair a faulty evolutionary operator using an LLM.\n",
        "    \"\"\"\n",
        "    if not clients:\n",
        "        print(\"Warning: No LLM clients configured. Cannot repair operator.\")\n",
        "        return faulty_code\n",
        "\n",
        "    client = random.choice(clients)\n",
        "    client_name = client.__class__.__module__.split('.')[0]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an expert Python programmer specializing in genetic algorithms. The following Python code for a `generate_next_population` function has failed with an error.\n",
        "\n",
        "Your task is to analyze the code and the stack trace to identify the bug and provide a corrected version of the function.\n",
        "\n",
        "**Faulty Code:**\n",
        "```python\n",
        "{faulty_code}\n",
        "```\n",
        "\n",
        "**Error Stack Trace:**\n",
        "```\n",
        "{error_trace}\n",
        "```\n",
        "\n",
        "Please provide only the corrected Python code block for the function. Do not include explanations or markdown formatting.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"--- Calling {client_name.capitalize()} API to repair operator ---\")\n",
        "        if client_name == 'google':\n",
        "            model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "            response = model.generate_content(prompt)\n",
        "            repaired_code = response.text\n",
        "        elif client_name == 'openai':\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that only returns Python code.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            repaired_code = response.choices[0].message.content\n",
        "\n",
        "        return repaired_code\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: API call to {client_name.capitalize()} for repair failed: {e}. Returning original faulty code.\")\n",
        "        return faulty_code\n",
        "\n",
        "def create_initial_operator_population(size, clients):\n",
        "    \"\"\"\n",
        "    Creates a diverse first generation of operators.\n",
        "    It starts with one seed function and uses the LLM to generate variations for the rest.\n",
        "    \"\"\"\n",
        "    print(f\"Creating a diverse initial population of size {size} using the LLM...\")\n",
        "\n",
        "    # The seed function remains the same standard GA\n",
        "    initial_evolution_code = \"\"\"\n",
        "\n",
        "def generate_next_population(current_population, fitness_scores, device, torch, SimpleNet):\n",
        "    '''\n",
        "    A standard Genetic Algorithm implementation for generating the next population.\n",
        "    It uses elitism, tournament selection, uniform crossover, and reset mutation.\n",
        "    '''\n",
        "    POPULATION_SIZE = len(current_population)\n",
        "    ELITISM_RATE = 0.05\n",
        "    TOURNAMENT_SIZE = 5\n",
        "\n",
        "    # --- Helper: Crossover ---\n",
        "    def crossover(parent1, parent2, device):\n",
        "        child = SimpleNet().to(device)\n",
        "        parent1_dict = parent1.state_dict()\n",
        "        parent2_dict = parent2.state_dict()\n",
        "        child_dict = child.state_dict()\n",
        "        for key in parent1_dict.keys():\n",
        "            mask = torch.randint(0, 2, size=parent1_dict[key].shape, device=device).float()\n",
        "            child_dict[key] = (parent1_dict[key] * mask) + (parent2_dict[key] * (1 - mask))\n",
        "        child.load_state_dict(child_dict)\n",
        "        return child\n",
        "\n",
        "    # --- Helper: Mutate ---\n",
        "    def mutate(model, device):\n",
        "        MUTATION_RESET_PROB = 0.00001\n",
        "        with torch.no_grad():\n",
        "            for param in model.parameters():\n",
        "                mask = torch.rand_like(param.data) < MUTATION_RESET_PROB\n",
        "                new_weights = torch.randn_like(param.data)\n",
        "                param.data[mask] = new_weights[mask]\n",
        "        return model\n",
        "\n",
        "    # --- Helper: Tournament Selection ---\n",
        "    def tournament_selection(population, scores):\n",
        "        tournament_indices = torch.randint(0, len(population), (TOURNAMENT_SIZE,))\n",
        "        winner_idx = tournament_indices[torch.argmax(scores[tournament_indices])]\n",
        "        return population[winner_idx]\n",
        "\n",
        "    # --- Main Generation Logic ---\n",
        "    sorted_indices = torch.argsort(fitness_scores, descending=True)\n",
        "    num_elite = int(POPULATION_SIZE * ELITISM_RATE)\n",
        "    new_population = [current_population[i] for i in sorted_indices[:num_elite]]\n",
        "\n",
        "    num_children_to_create = POPULATION_SIZE - num_elite\n",
        "    for _ in range(num_children_to_create):\n",
        "        parent1 = tournament_selection(current_population, fitness_scores)\n",
        "        parent2 = tournament_selection(current_population, fitness_scores)\n",
        "        child = crossover(parent1, parent2, device)\n",
        "        child = mutate(child, device)\n",
        "        new_population.append(child)\n",
        "\n",
        "    return new_population\n",
        "\"\"\"\n",
        "    # Create the first individual from the seed code\n",
        "    seed_individual = OperatorIndividual(initial_evolution_code)\n",
        "\n",
        "    # Initialize the population with the seed\n",
        "    population = [seed_individual]\n",
        "\n",
        "    # Use the LLM to generate the rest of the initial population for diversity\n",
        "    if size > 1:\n",
        "        print(f\"Generating {size - 1} initial variations from the seed operator...\")\n",
        "        # A progress bar is helpful here since this involves multiple API calls\n",
        "        for _ in tqdm(range(size - 1), desc=\"Bootstrapping Initial Population\"):\n",
        "            evolved_individual = llm_evolve_operator(seed_individual, clients)\n",
        "            population.append(evolved_individual)\n",
        "\n",
        "    print(\"Initial population created.\")\n",
        "    return population\n",
        "\n",
        "def selection(population, tournament_size=3):\n",
        "    \"\"\"Selects a parent from the population using tournament selection.\"\"\"\n",
        "    if len(population) < tournament_size:\n",
        "        return random.choice(population)\n",
        "    tournament = random.sample(population, tournament_size)\n",
        "    tournament.sort(key=lambda x: x.fitness, reverse=True)\n",
        "    return tournament[0]\n",
        "\n",
        "\n",
        "# @title 6. Fitness Evaluation: Running the Inner GA\n",
        "def run_inner_ga_for_fitness(operator_individual, train_loader, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluates an OperatorIndividual by running a GA using its evolved operator.\n",
        "    This version captures detailed per-generation statistics (best, avg, worst fitness)\n",
        "    for each run to analyze the operator's behavior over time.\n",
        "    \"\"\"\n",
        "    # --- Parameters ---\n",
        "    NUM_EVAL_RUNS = 3 # Run the inner GA 3 times to average out randomness\n",
        "    # --- Inner GA Parameters ---\n",
        "    POPULATION_SIZE = 50\n",
        "    GENERATIONS = 10\n",
        "    BATCHES_PER_GENERATION = 5\n",
        "    MAX_REPAIR_ATTEMPTS = 1\n",
        "    for attempt in range(MAX_REPAIR_ATTEMPTS + 1):\n",
        "        try:\n",
        "            local_scope = {}\n",
        "            exec(operator_individual.evolution_operator_code, {}, local_scope)\n",
        "            generate_next_population_fn = local_scope['generate_next_population']\n",
        "            # If exec is successful, break the loop\n",
        "            break\n",
        "        except Exception as e:\n",
        "            if attempt < MAX_REPAIR_ATTEMPTS:\n",
        "                print(f\"Error executing evolved code. Attempting LLM repair ({attempt + 1}/{MAX_REPAIR_ATTEMPTS}).\")\n",
        "                error_trace = traceback.format_exc()\n",
        "                repaired_code = repair_operator_with_llm(operator_individual.evolution_operator_code, error_trace, clients)\n",
        "                operator_individual.evolution_operator_code = repaired_code\n",
        "            else:\n",
        "                print(f\"Error executing evolved code after repair attempts: {e}\")\n",
        "                return None # Return None on final failure\n",
        "\n",
        "    all_runs_histories = []\n",
        "    print(f\"Starting {NUM_EVAL_RUNS} detailed evaluation runs for this operator...\")\n",
        "\n",
        "    for run_num in range(NUM_EVAL_RUNS):\n",
        "        print(f\"  > Run {run_num + 1}/{NUM_EVAL_RUNS}...\")\n",
        "        # --- Inner GA Execution ---\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        population = [SimpleNet().to(DEVICE) for _ in range(POPULATION_SIZE)]\n",
        "        streams = [torch.cuda.Stream() for _ in range(POPULATION_SIZE)] if torch.cuda.is_available() else []\n",
        "        train_iterator = iter(itertools.cycle(train_loader))\n",
        "\n",
        "        run_history = []\n",
        "\n",
        "        for generation in range(GENERATIONS):\n",
        "            fitness_scores = torch.zeros(POPULATION_SIZE, device=DEVICE)\n",
        "            for _ in range(BATCHES_PER_GENERATION):\n",
        "                images, labels = next(train_iterator)\n",
        "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "                with torch.no_grad():\n",
        "                    if torch.cuda.is_available():\n",
        "                        for i, model in enumerate(population):\n",
        "                            with torch.cuda.stream(streams[i]):\n",
        "                                outputs = model(images)\n",
        "                                loss = loss_fn(outputs, labels)\n",
        "                                fitness_scores[i] += -loss # Fitness is negative loss\n",
        "                    else: # CPU fallback\n",
        "                         for i, model in enumerate(population):\n",
        "                            outputs = model(images)\n",
        "                            loss = loss_fn(outputs, labels)\n",
        "                            fitness_scores[i] += -loss\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            # ✨ NEW: Track detailed stats for this generation ✨\n",
        "            best_fitness_gen = fitness_scores.max().item()\n",
        "            worst_fitness_gen = fitness_scores.min().item()\n",
        "            avg_fitness_gen = fitness_scores.mean().item()\n",
        "            run_history.append({\n",
        "                \"generation\": generation,\n",
        "                \"best\": best_fitness_gen,\n",
        "                \"avg\": avg_fitness_gen,\n",
        "                \"worst\": worst_fitness_gen\n",
        "            })\n",
        "\n",
        "            # --- Evolve the population using the LLM's operator ---\n",
        "            population = generate_next_population_fn(population, fitness_scores, DEVICE, torch, SimpleNet)\n",
        "\n",
        "        all_runs_histories.append(run_history)\n",
        "\n",
        "        # We calculate final test accuracy to report progress, but it is NOT the primary fitness metric.\n",
        "        # The generational dynamics (run_history) are the key feedback for the LLM.\n",
        "        best_model = population[torch.argmax(fitness_scores)]\n",
        "        final_acc = evaluate_nn_model(best_model, test_loader)\n",
        "        print(f\"    Run {run_num + 1} finished. Final test accuracy of best model: {final_acc:.2f}%\")\n",
        "\n",
        "    # The detailed history is now the primary output of the evaluation\n",
        "    return all_runs_histories\n",
        "\n",
        "\n",
        "# @title 7. Run the Main Meta-Evolutionary Loop\n",
        "def main():\n",
        "    \"\"\"The main function to run the outer (meta) EA.\"\"\"\n",
        "    if not llm_clients:\n",
        "        print(\"Fatal: No LLM clients were configured. Please add API keys to Colab secrets and restart the runtime.\")\n",
        "        return\n",
        "\n",
        "    # --- Meta-EA Parameters ---\n",
        "    META_POPULATION_SIZE = 6\n",
        "    META_GENERATIONS = 4\n",
        "    MUTATION_RATE = 0.5\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"Warning: CUDA not available. This script is very slow on CPU.\")\n",
        "\n",
        "    train_loader, test_loader = get_data_loaders()\n",
        "\n",
        "    operator_population = create_initial_operator_population(META_POPULATION_SIZE, llm_clients)\n",
        "\n",
        "    for gen in range(META_GENERATIONS):\n",
        "        print(f\"\\n{'='*25} META-GENERATION {gen + 1}/{META_GENERATIONS} {'='*25}\")\n",
        "        print(\"Evolving the `generate_next_population` operator...\")\n",
        "\n",
        "        for i, individual in enumerate(operator_population):\n",
        "                # Only evaluate individuals that have no history\n",
        "                if not individual.run_histories:\n",
        "                    print(f\"\\n--- Evaluating Operator Individual {i+1}/{META_POPULATION_SIZE} ---\")\n",
        "                    display(Markdown(f\"```python\\n{individual.evolution_operator_code}\\n```\"))\n",
        "                    # The function now returns a list of detailed run histories\n",
        "                    new_histories = run_inner_ga_for_fitness(individual, train_loader, test_loader, llm_clients)\n",
        "\n",
        "                    if new_histories is None:\n",
        "                        # Assign negative infinity fitness on failure\n",
        "                        individual.fitness = -float('inf')\n",
        "                        print(f\"Finished evaluation. Operator {i+1} failed and was assigned a fitness of -inf.\")\n",
        "                    else:\n",
        "                        # Use the new method to update the individual's history and fitness\n",
        "                        individual.update_history(new_histories)\n",
        "                        print(f\"Finished evaluation. Operator {i+1} primary fitness: {individual.fitness:.4f}\")\n",
        "\n",
        "        # Sort by the primary fitness metric for elitism\n",
        "        operator_population.sort(key=lambda x: x.fitness, reverse=True)\n",
        "\n",
        "        print(f\"\\n--- Meta-Generation {gen+1} Results ---\")\n",
        "        best_op = operator_population[0]\n",
        "        print(f\"Best Operator Fitness (Avg Final Best): {best_op.fitness:.4f}\")\n",
        "        print(\"Best Performing Operator's Code:\")\n",
        "        display(Markdown(f\"```python\\n{best_op.evolution_operator_code}\\n```\"))\n",
        "        print(\"Best Operator's Performance Summary:\")\n",
        "        display(Markdown(best_op.get_performance_summary_text()))\n",
        "\n",
        "\n",
        "        next_generation = []\n",
        "        next_generation.append(best_op) # Elitism\n",
        "\n",
        "        while len(next_generation) < META_POPULATION_SIZE:\n",
        "            parent = selection(operator_population)\n",
        "            if random.random() < MUTATION_RATE:\n",
        "                child = llm_evolve_operator(parent, llm_clients)\n",
        "            else:\n",
        "                # If not mutating, create a fresh copy with the parent's history\n",
        "                child = OperatorIndividual(parent.evolution_operator_code)\n",
        "                child.run_histories = parent.run_histories[:] # Copy history\n",
        "                child.fitness = parent.fitness\n",
        "            next_generation.append(child)\n",
        "\n",
        "        operator_population = next_generation\n",
        "\n",
        "    print(\"\\nMeta-Evolution finished!\")\n",
        "    print(\"Final Best Performing Operator:\")\n",
        "    display(Markdown(str(operator_population[0])))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3bd508399316412bb427688f4db302f4",
            "e1a3e12f96c94980930ff7a3f2166360",
            "a7b21fe2bcf0482780a00162a4cafd96",
            "c1da06397f0c4880a3cd8f794d52d254",
            "c83d8efffdfb4327985cd41be9a65506",
            "595f22739d2842e692e8f89123964620",
            "22d69547512d4ba59e17bf2061333dc0",
            "4be0744ea1e74d52918bbc8738cea93b",
            "852841068c4b41dcbc7285d7a7ba9911",
            "6d4ada05f351462a935bb67ebbbb8fdf",
            "40a56e2849914920bcf1e4bfa478e5e2"
          ]
        },
        "id": "ERYD0PlYVHWG",
        "outputId": "ef6c4364-47f0-4eee-f9f5-134eb2001168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully configured and added Gemini client.\n",
            "\n",
            "--- Available Gemini Models (for 'generateContent') ---\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-flash-lite-preview-06-17\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.5-pro-preview-06-05\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "-----------------------------------------------------\n",
            "\n",
            "✅ Successfully configured and added OpenAI client.\n",
            "\n",
            "--- Available OpenAI Models (GPT models) ---\n",
            "chatgpt-4o-latest\n",
            "gpt-3.5-turbo\n",
            "gpt-3.5-turbo-0125\n",
            "gpt-3.5-turbo-1106\n",
            "gpt-3.5-turbo-16k\n",
            "gpt-3.5-turbo-instruct\n",
            "gpt-3.5-turbo-instruct-0914\n",
            "gpt-4\n",
            "gpt-4-0125-preview\n",
            "gpt-4-0613\n",
            "gpt-4-1106-preview\n",
            "gpt-4-turbo\n",
            "gpt-4-turbo-2024-04-09\n",
            "gpt-4-turbo-preview\n",
            "gpt-4.1\n",
            "gpt-4.1-2025-04-14\n",
            "gpt-4.1-mini\n",
            "gpt-4.1-mini-2025-04-14\n",
            "gpt-4.1-nano\n",
            "gpt-4.1-nano-2025-04-14\n",
            "gpt-4o\n",
            "gpt-4o-2024-05-13\n",
            "gpt-4o-2024-08-06\n",
            "gpt-4o-2024-11-20\n",
            "gpt-4o-audio-preview\n",
            "gpt-4o-audio-preview-2024-10-01\n",
            "gpt-4o-audio-preview-2024-12-17\n",
            "gpt-4o-audio-preview-2025-06-03\n",
            "gpt-4o-mini\n",
            "gpt-4o-mini-2024-07-18\n",
            "gpt-4o-mini-audio-preview\n",
            "gpt-4o-mini-audio-preview-2024-12-17\n",
            "gpt-4o-mini-realtime-preview\n",
            "gpt-4o-mini-realtime-preview-2024-12-17\n",
            "gpt-4o-mini-search-preview\n",
            "gpt-4o-mini-search-preview-2025-03-11\n",
            "gpt-4o-mini-transcribe\n",
            "gpt-4o-mini-tts\n",
            "gpt-4o-realtime-preview\n",
            "gpt-4o-realtime-preview-2024-10-01\n",
            "gpt-4o-realtime-preview-2024-12-17\n",
            "gpt-4o-realtime-preview-2025-06-03\n",
            "gpt-4o-search-preview\n",
            "gpt-4o-search-preview-2025-03-11\n",
            "gpt-4o-transcribe\n",
            "gpt-5\n",
            "gpt-5-2025-08-07\n",
            "gpt-5-chat-latest\n",
            "gpt-5-mini\n",
            "gpt-5-mini-2025-08-07\n",
            "gpt-5-nano\n",
            "gpt-5-nano-2025-08-07\n",
            "gpt-audio\n",
            "gpt-audio-2025-08-28\n",
            "gpt-image-1\n",
            "gpt-realtime\n",
            "gpt-realtime-2025-08-28\n",
            "-------------------------------------------\n",
            "\n",
            "Using device: cuda\n",
            "Creating a diverse initial population of size 6 using the LLM...\n",
            "Generating 5 initial variations from the seed operator...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Bootstrapping Initial Population:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3bd508399316412bb427688f4db302f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Calling Openai API to evolve operator (with generational feedback) ---\n",
            "--- Calling Google API to evolve operator (with generational feedback) ---\n",
            "--- Calling Google API to evolve operator (with generational feedback) ---\n",
            "--- Calling Openai API to evolve operator (with generational feedback) ---\n",
            "--- Calling Openai API to evolve operator (with generational feedback) ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3bd508399316412bb427688f4db302f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1a3e12f96c94980930ff7a3f2166360",
              "IPY_MODEL_a7b21fe2bcf0482780a00162a4cafd96",
              "IPY_MODEL_c1da06397f0c4880a3cd8f794d52d254"
            ],
            "layout": "IPY_MODEL_c83d8efffdfb4327985cd41be9a65506"
          }
        },
        "e1a3e12f96c94980930ff7a3f2166360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595f22739d2842e692e8f89123964620",
            "placeholder": "​",
            "style": "IPY_MODEL_22d69547512d4ba59e17bf2061333dc0",
            "value": "Bootstrapping Initial Population:  80%"
          }
        },
        "a7b21fe2bcf0482780a00162a4cafd96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4be0744ea1e74d52918bbc8738cea93b",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_852841068c4b41dcbc7285d7a7ba9911",
            "value": 4
          }
        },
        "c1da06397f0c4880a3cd8f794d52d254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d4ada05f351462a935bb67ebbbb8fdf",
            "placeholder": "​",
            "style": "IPY_MODEL_40a56e2849914920bcf1e4bfa478e5e2",
            "value": " 4/5 [03:16&lt;00:49, 49.56s/it]"
          }
        },
        "c83d8efffdfb4327985cd41be9a65506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "595f22739d2842e692e8f89123964620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22d69547512d4ba59e17bf2061333dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4be0744ea1e74d52918bbc8738cea93b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "852841068c4b41dcbc7285d7a7ba9911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d4ada05f351462a935bb67ebbbb8fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40a56e2849914920bcf1e4bfa478e5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}